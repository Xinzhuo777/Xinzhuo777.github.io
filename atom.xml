<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>machine learning</title>
  
  
  <link href="http://xinzhuo777.github.io/atom.xml" rel="self"/>
  
  <link href="http://xinzhuo777.github.io/"/>
  <updated>2021-12-22T13:19:26.900Z</updated>
  <id>http://xinzhuo777.github.io/</id>
  
  <author>
    <name>Xinzhuo Li</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>windows转mac体验与安装</title>
    <link href="http://xinzhuo777.github.io/2021/12/22/windows%E8%BD%ACmac%E4%BD%93%E9%AA%8C%E4%B8%8E%E5%AE%89%E8%A3%85/"/>
    <id>http://xinzhuo777.github.io/2021/12/22/windows%E8%BD%ACmac%E4%BD%93%E9%AA%8C%E4%B8%8E%E5%AE%89%E8%A3%85/</id>
    <published>2021-12-22T13:17:31.000Z</published>
    <updated>2021-12-22T13:19:26.900Z</updated>
    
    <content type="html"><![CDATA[<h2 id="由windows转向mac的全新体验与搭建环境"><a href="#由windows转向mac的全新体验与搭建环境" class="headerlink" title="由windows转向mac的全新体验与搭建环境"></a>由windows转向mac的全新体验与搭建环境</h2><p>最近多了台MacBook，然后使用体验还行，符合我的需求，但是软件适配确实有点小问题，体现在mac系统没有一个足够好用免费的ssh工具，不过这个也无伤大雅。整体的体验还是非常不错，续航一流。安装软件和配置环境上手很快。</p><p>这里讲几个我搭建环境时遇到的小坑。</p><h3 id="本地mac配置ssh连接github"><a href="#本地mac配置ssh连接github" class="headerlink" title="本地mac配置ssh连接github"></a>本地mac配置ssh连接github</h3><p>mac本身安装homebrew后基本的环境就有了。要连接github需要生成一个ssh相关的密钥</p><p>mac terminal指令为:ssh-keygen -t rsa -C “gerenyouxiang@example.com”</p><p>这个指令会生成两个文件 id_rsa and id_rsa.pub 这两个文件需要在终端输入 open ～/.ssh才能看见。将文件生成在其他地方，github会不许可（我也不知道为什么）。然后就是在github上操作。最后在本地终端输入 ：ssh git@github.com </p><p>返回结果是 ：hi，you’ve successfully authenticated 之类的话。</p><p>然后配合sourcetree 应该是能成功的，还没试，手头上没有很强的代码。</p><h3 id="新mac如何移植原来的老博客"><a href="#新mac如何移植原来的老博客" class="headerlink" title="新mac如何移植原来的老博客"></a>新mac如何移植原来的老博客</h3><p>mac本身查看一下node -v 发现环境没有，于是brew install node。配置完环境 ，然后要安装npm install hexo-cli - g</p><p>在我原来的笔记本上拷下几个文件</p><p>config.yml</p><p>Package.json</p><p>scaffolds</p><p>source</p><p>themes</p><p>以上5个文件</p><p>然后在这里由于我笔记本磁盘只有512G。我未来想好好认真的写写技术博客总结一下自己，笔记本应该会用很久，为了省空间，把这个放在了移动硬盘里。安装hexo也是在移动硬盘上。</p><p>进入移动硬盘 然后执行以下操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br><span class="line">npm install hexo-deployer-git --save  &#x2F;&#x2F; 文章部署到 git 的模块</span><br><span class="line">（下面为选择安装）</span><br><span class="line">npm install hexo-generator-feed --save  &#x2F;&#x2F; 建立 RSS 订阅</span><br><span class="line">npm install hexo-generator-sitemap --save &#x2F;&#x2F; 建立站点地图</span><br></pre></td></tr></table></figure><p>我这里卡了一会儿，不过应该正常。然后马上发下博客试一下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;由windows转向mac的全新体验与搭建环境&quot;&gt;&lt;a href=&quot;#由windows转向mac的全新体验与搭建环境&quot; class=&quot;headerlink&quot; title=&quot;由windows转向mac的全新体验与搭建环境&quot;&gt;&lt;/a&gt;由windows转向mac的全新体</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://xinzhuo777.github.io/2021/12/22/hello-world/"/>
    <id>http://xinzhuo777.github.io/2021/12/22/hello-world/</id>
    <published>2021-12-22T06:46:33.000Z</published>
    <updated>2020-07-06T03:51:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>综合实践</title>
    <link href="http://xinzhuo777.github.io/2021/04/23/%E7%BB%BC%E5%90%88%E5%AE%9E%E8%B7%B5/"/>
    <id>http://xinzhuo777.github.io/2021/04/23/%E7%BB%BC%E5%90%88%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-04-23T15:13:51.000Z</published>
    <updated>2021-04-23T15:36:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>综合实践</p><p>大部分东西前面都有所实践了</p><p>但是本人又没有合适的项目运用docker来部署，就那docker来部署一些小文件吧</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagefinal1_1.png?raw=true"></p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagefinal1_2.png?raw=true"></p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagefinal1_3.png?raw=true"></p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagefinal1_4.png?raw=true"></p><p>更加高级的操作就没有尝试了。。。事情实在太多，还有考试。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;综合实践&lt;/p&gt;
&lt;p&gt;大部分东西前面都有所实践了&lt;/p&gt;
&lt;p&gt;但是本人又没有合适的项目运用docker来部署，就那docker来部署一些小文件吧&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/Xinzhuo777/picgo/blob/mast</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>compose</title>
    <link href="http://xinzhuo777.github.io/2021/04/21/compose/"/>
    <id>http://xinzhuo777.github.io/2021/04/21/compose/</id>
    <published>2021-04-21T14:25:05.000Z</published>
    <updated>2021-04-21T14:39:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="docker-compose使用"><a href="#docker-compose使用" class="headerlink" title="docker compose使用"></a>docker compose使用</h1><p>首先要pull docker-compose的镜像</p><p>然后按照教程，来进行操作</p><p>我创建了一个test-compose文件夹，cd进入文件夹内，进行touch Dockerfile </p><p>之后vim Dockerfile </p><p>之后vim docker-compose.yml文件</p><p>按照教程输入指令</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagephoto1_1.png?raw=true"></p><p>结果并不能打开这个网址</p><p>经过测试，问题原因在于并没有端口映射，在容器里的端口没有反馈到宿主机上，因此没有实现docker compose功能</p><p>经过docker docker run -p 8000:80 -it test-compose /bin/bash映射，成功解决问题</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagephoto1_2.png?raw=true"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;docker-compose使用&quot;&gt;&lt;a href=&quot;#docker-compose使用&quot; class=&quot;headerlink&quot; title=&quot;docker compose使用&quot;&gt;&lt;/a&gt;docker compose使用&lt;/h1&gt;&lt;p&gt;首先要pull docker</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Docker网络管理</title>
    <link href="http://xinzhuo777.github.io/2021/04/19/Docker%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/"/>
    <id>http://xinzhuo777.github.io/2021/04/19/Docker%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/</id>
    <published>2021-04-19T04:37:28.000Z</published>
    <updated>2021-04-19T13:15:22.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker网络"><a href="#Docker网络" class="headerlink" title="Docker网络"></a>Docker网络</h1><h2 id="端口映射"><a href="#端口映射" class="headerlink" title="端口映射"></a>端口映射</h2><p>在启动容器的时候，如果不指定对应的端口，在容器外将无法通过网络来访问容器内的服务。Docker提供端口映射机制来将容器内的服务提供给外部网络访问，实质上就是将宿主机的端口映射到容器中，使得外部网络访问主机的端口便可访问容器内的服务。</p><p>1..实现端口映射，需要在运行docker run命令时使用-P（大写）选项实现随机映射，Docker会随机映射一个端口范围在49000~49900的端口到容器内部开放的网络端口。</p><p>2.查看本机的端口是否映射到了容器中的端口<br><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_1.png?raw=true"></p><p>3.还可以在运行docker run命令时使用-p（小写）选项指定要映射的端口。如：</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_2.png?raw=true"></p><h2 id="容器互联—link指令"><a href="#容器互联—link指令" class="headerlink" title="容器互联—link指令"></a>容器互联—link指令</h2><p>容器互联就是通过容器的名称在容器间建立一条专门的网络通信隧道从而实现容器的互联。</p><p>注意：如果已经命名了一个相同的容器，当要再次使用这个名称时，需要使用docker rm命令删除之前创建的同名容器。</p><p>1.创建源容器</p><p>2.创建接收容器</p><p>此处用—link来指定连接容器实现互联</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_4.png?raw=true"></p><p>3.测试容器互联</p><p>利用ping命令查看容器web2是否能相互连通</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_5.png?raw=true"></p><h2 id="容器互联"><a href="#容器互联" class="headerlink" title="容器互联"></a>容器互联</h2><h3 id="新建网络"><a href="#新建网络" class="headerlink" title="新建网络"></a>新建网络</h3><p>下面先创建一个新的Docker网络</p><p>docker network creat -d bridge my-net</p><h3 id="连接容器"><a href="#连接容器" class="headerlink" title="连接容器"></a>连接容器</h3><p>再打开新的终端，再运行一个容器并加入到my-net网络</p><p>docker run -it —rm —name busybox1 —network my-net busybox sh</p><p>docker run -it —rm —name busybox2 —network my-net busybox sh</p><p>可以用docker container ls</p><p>用ping来测试两个容器是否连接</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_6.png?raw=true"><br><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_7.png?raw=true"><br><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_8.png?raw=true"><br><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_9.png?raw=true"></p><h2 id="配置DNS"><a href="#配置DNS" class="headerlink" title="配置DNS"></a>配置DNS</h2><p>关于配置容器的主机名和DNS。秘诀就是Docker利用虚拟文件来挂载容器的3个相关配置文件</p><p>在容器中使用mount命令可以看到挂载信息</p><p>mount指令的作用是让宿主主机DNS信息发生更新后，所有Docker容器DNS配置通过 /etc/resolv.conf文件立刻得到更新。</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_10.png?raw=true"></p><p>配置全部容器的DNS，也可以在/etc/docker/daemon.json文件中增加以下内容来设置。</p><p>我们可以通过以下指令来证明命令生效：</p><p>docker run -it —rm [主机名]  cat etc/resolv.conf</p><h3 id="Docker的网络模式"><a href="#Docker的网络模式" class="headerlink" title="Docker的网络模式"></a>Docker的网络模式</h3><p>docker network ls 查看网络，默认创建三种网络</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_11.png?raw=true"></p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepage1_12.png?raw=true"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker网络&quot;&gt;&lt;a href=&quot;#Docker网络&quot; class=&quot;headerlink&quot; title=&quot;Docker网络&quot;&gt;&lt;/a&gt;Docker网络&lt;/h1&gt;&lt;h2 id=&quot;端口映射&quot;&gt;&lt;a href=&quot;#端口映射&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Docker数据管理</title>
    <link href="http://xinzhuo777.github.io/2021/04/16/text1/"/>
    <id>http://xinzhuo777.github.io/2021/04/16/text1/</id>
    <published>2021-04-16T14:29:42.000Z</published>
    <updated>2021-04-16T14:56:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker的数据管理"><a href="#Docker的数据管理" class="headerlink" title="Docker的数据管理"></a>Docker的数据管理</h1><p>管理Docker容器中的数据主要有两种方式：数据卷和数据卷容器</p><h2 id="数据卷"><a href="#数据卷" class="headerlink" title="数据卷"></a>数据卷</h2><p>1.创建数据卷</p><p>在docker run -v选项中可以在容器内创建数据卷。多次使用-v可以创建多个数据卷。使用—name选项可以给容器创建一个友好的自定义名称。</p><p>（1）在centos镜像中创建一个web1容器，同时将宿主机的/var/www目录挂载到容器的/data1目录上</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageproject1_3.png?raw=true"></p><p>（2）在宿主机本地/var/www目录中创建一个文件，进入运行的容器中，在相应的挂载目录下可以看到刚才在宿主机上创建的文件，实现了从宿主机到容器的数据迁移。</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageproject1_5.png?raw=true"></p><h2 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h2><p>（1）创建个新的数据卷容器为web100，其中所创建的数据卷分别挂载到了/data1与/data2目录上。</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageproject1_1.png?raw=true"></p><p>（2）使用—volumes-from 来挂载web100容器中的数据卷到新的容器，新的容器名为db1。</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageproject1_2.png?raw=true"></p><p>（3）在db1容器数据卷/data2目录中创建一个文件file。在web100容器中的/data2目录中可以查看到文件file。</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageproject1_4.png?raw=true"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker的数据管理&quot;&gt;&lt;a href=&quot;#Docker的数据管理&quot; class=&quot;headerlink&quot; title=&quot;Docker的数据管理&quot;&gt;&lt;/a&gt;Docker的数据管理&lt;/h1&gt;&lt;p&gt;管理Docker容器中的数据主要有两种方式：数据卷和数据卷容器&lt;/p</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Docker容器与镜像</title>
    <link href="http://xinzhuo777.github.io/2021/04/14/docker%E5%AE%B9%E5%99%A8%E4%B8%8E%E9%95%9C%E5%83%8F/"/>
    <id>http://xinzhuo777.github.io/2021/04/14/docker%E5%AE%B9%E5%99%A8%E4%B8%8E%E9%95%9C%E5%83%8F/</id>
    <published>2021-04-14T14:17:17.000Z</published>
    <updated>2021-04-15T15:11:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker容器与镜像"><a href="#Docker容器与镜像" class="headerlink" title="Docker容器与镜像"></a>Docker容器与镜像</h1><h2 id="Docker基本的容器指令"><a href="#Docker基本的容器指令" class="headerlink" title="Docker基本的容器指令"></a>Docker基本的容器指令</h2><h4 id="创建交互式容器"><a href="#创建交互式容器" class="headerlink" title="创建交互式容器"></a>创建交互式容器</h4><p>docker run -i-t —name =c1 centos /bin/bash</p><p>-i:交互式容器</p><p>-t: tty终端（分配一个终端，操作容器）</p><p>注意centos 后面需要空格和/bin分开</p><h4 id="启动和终止容器"><a href="#启动和终止容器" class="headerlink" title="启动和终止容器"></a>启动和终止容器</h4><p>docker  container[ID]  start/stop</p><p>docker ps 列出容器，默认列出只在运行的容器；加-a可以显示所有的容器，包括未运行的容器<br><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageweek1_8.png?raw=true"></p><h4 id="后台运行容器"><a href="#后台运行容器" class="headerlink" title="后台运行容器"></a>后台运行容器</h4><p>docker run -d centos /bin/bash</p><h4 id="进入容器的两种办法"><a href="#进入容器的两种办法" class="headerlink" title="进入容器的两种办法"></a>进入容器的两种办法</h4><p>docker attach container[ID]</p><p>docker exec -it centos  /bin/bash</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageweek1_11.png?raw=true"></p><h4 id="批量删除容器"><a href="#批量删除容器" class="headerlink" title="批量删除容器"></a>批量删除容器</h4><p>docker rm -v $(docker ps -aq -f status=exited)<br><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageweek1_6.png?raw=true"></p><h4 id="查看容器信息"><a href="#查看容器信息" class="headerlink" title="查看容器信息"></a>查看容器信息</h4><p>docker inspect container[id]</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageweek1_9.png?raw=true"></p><h2 id="Dockerfile制作"><a href="#Dockerfile制作" class="headerlink" title="Dockerfile制作"></a>Dockerfile制作</h2><p>首先要明确一点Dockerfile并不一定要用vim编辑，也可以在本地制作之后上传到服务器</p><p>此处先介绍vim编辑的方法</p><p>mkdir mynginx</p><p>cd mynginx</p><p>touch Dockerfile</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageweek1_12.png?raw=true"></p><p>mkdir 相当于创建了一个新目录，理论上要取任何名字对于你的这个Dockerfile没影响</p><p>建立一个文本文件，并命名为 Dockerfile</p><p>此处要进入vim编辑</p><p>在vim编辑模式中要严格按照一些基本的格式</p><p>FROM指定基础镜像</p><p>所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。基础镜像是必须指定的。而 <code>FROM</code> 就是指定 <strong>基础镜像</strong>，因此一个 <code>Dockerfile</code> 中 <code>FROM</code> 是必备的指令，并且必须是第一条指令</p><p><code>RUN</code> 指令是用来执行命令行命令的。由于命令行的强大能力，<code>RUN</code> 指令在定制镜像时是最常用的指令之一。其格式有两种：</p><p><em>shell</em> 格式：RUN &lt;命令&gt;</p><p>exec格式：RUN [“可执行文件”, “参数1”, “参数2”]</p><h4 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h4><p>docker build [选项] &lt;上下文路径/URL/-&gt;</p><p>• [OPTIONS]：通常指令包括-t，用来指定image的名字。-f指定Dockfile的上下文路径。</p><p>•上下文路径|URL：上下文路径，如果只有一个小圆点 “.” 代表当前目录。 </p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imageweek1_10.png?raw=true"></p><h4 id="启动容器，运行镜像"><a href="#启动容器，运行镜像" class="headerlink" title="启动容器，运行镜像"></a>启动容器，运行镜像</h4><p>docker run -it -p 6500:8000 -v /home/code/webtest:/code —name web —restart always —privileged=true webtest</p><p> -p：把容器的8000端口映射到宿主机6500</p><p> -v：主机的目录/home/code/webtest映射到容器的目录/code</p><p>  —name：给容器起个名字web，webtest是我们刚刚构建的镜像</p><p>  —restart：always 容器退出时总是重启</p><p>  —privileged=true：执行容器内文件需要的权限</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker容器与镜像&quot;&gt;&lt;a href=&quot;#Docker容器与镜像&quot; class=&quot;headerlink&quot; title=&quot;Docker容器与镜像&quot;&gt;&lt;/a&gt;Docker容器与镜像&lt;/h1&gt;&lt;h2 id=&quot;Docker基本的容器指令&quot;&gt;&lt;a href=&quot;#Dock</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Docker基本情况与安装</title>
    <link href="http://xinzhuo777.github.io/2021/04/12/Docker%E5%9F%BA%E6%9C%AC%E6%83%85%E5%86%B5%E4%B8%8E%E5%AE%89%E8%A3%85/"/>
    <id>http://xinzhuo777.github.io/2021/04/12/Docker%E5%9F%BA%E6%9C%AC%E6%83%85%E5%86%B5%E4%B8%8E%E5%AE%89%E8%A3%85/</id>
    <published>2021-04-12T11:26:05.000Z</published>
    <updated>2021-04-16T15:40:48.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Docker基本情况与安装"><a href="#Docker基本情况与安装" class="headerlink" title="Docker基本情况与安装"></a>Docker基本情况与安装</h1><h2 id="Docker的基本情况"><a href="#Docker的基本情况" class="headerlink" title="Docker的基本情况"></a>Docker的基本情况</h2><p>Docker的英文翻译是“码头工人”，即搬运工，它搬运的东西就是我们常说的集装箱Container，Container里面装的是任意类型的App。我们的开发人员可以通过Docker将App变成一种标准的、可移植的、自管理的组件，我们可以在任何主流的操作系统中开发、调试和运行。</p><p>从概念上来看，Docker和传统的虚拟机比较类似，只是更轻量级，更方便使用。Docker和虚拟机最主要的区别有以下几点：</p><ul><li>虚拟化技术依赖的是物理CPU和内存，是硬件级别的；Docker是构建在操作系统层面的，复用操作系统的容器化技术，所以Docker同样可以运行在虚拟机上面。</li><li>虚拟机中的操作系统是一个完整的操作系统镜像，比较复杂；而Docker比较轻量级，我们可以用Docker部署一个独立的redis，就类似于在虚拟机当中安装一个redis应用，但Docker部署的应用是完全隔离的。</li><li>传统的虚拟机技术是通过快照来保存状态的；而Docker引入了类似于源码管理的机制，将容器历史版本一一记录下来，切换成本非常之低。</li><li>传统的虚拟化技术在构建系统的时候非常复杂；而Docker可以通过一个简单的Dockerfile文件来构建整个容器，更重要的是Dockerfile可以手动编写，这样应用程序开发都可以通过发布Dockerfile来定义应用的环境和依赖，对于持续交付非常有利。</li></ul><p>Docker运行在物理机上与运行在虚拟机上的对比：</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagezero.png?raw=true"></p><p>Docker的特性：</p><ul><li><p>标准化</p><ul><li>保证一致的运行环境</li><li>弹性伸缩，快速扩容</li><li>方便迁移</li><li>持续集成、持续交付与持续部署</li></ul></li><li><p>高性能</p><ul><li>不需要进行硬件虚拟以及运行完整的操作系统</li></ul></li><li><p>轻量级</p><ul><li>快速启动</li></ul></li><li><p>隔离性</p><ul><li>进程隔离</li></ul></li></ul><h2 id="Docker安装过程"><a href="#Docker安装过程" class="headerlink" title="Docker安装过程"></a>Docker安装过程</h2><p>参看<a href="https://hub.docker.com" target="_blank" rel="noopener">https://hub.docker.com</a></p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/image%7BN%5BDU%7D(I%7BCH%25%5D)9R6W%7DN%60I3.png?raw=true"></p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepig1.png?raw=true"><br>其中从镜像拉取docker要核对一下这个验证码</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepig2.png?raw=true"></p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepig3.png?raw=true"><br>配置镜像加速器（本人采取的是阿里云的镜像加速器）</p><p>服务器版本是centos</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepig5.png?raw=true"><br>最后配置完成以后，通过docker pull 来拉取镜像</p><p><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepig4.png?raw=true"><br><img src="https://github.com/Xinzhuo777/picgo/blob/master/imagepig6.png?raw=true"></p><p>总体还是比较简单的！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Docker基本情况与安装&quot;&gt;&lt;a href=&quot;#Docker基本情况与安装&quot; class=&quot;headerlink&quot; title=&quot;Docker基本情况与安装&quot;&gt;&lt;/a&gt;Docker基本情况与安装&lt;/h1&gt;&lt;h2 id=&quot;Docker的基本情况&quot;&gt;&lt;a href=</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>[graphviz]</title>
    <link href="http://xinzhuo777.github.io/2020/09/08/%E8%BF%90%E7%94%A8graphviz%E5%AF%B9%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://xinzhuo777.github.io/2020/09/08/%E8%BF%90%E7%94%A8graphviz%E5%AF%B9%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%AF%E8%A7%86%E5%8C%96/</id>
    <published>2020-09-08T12:55:09.000Z</published>
    <updated>2020-09-09T05:21:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="运用graphviz对决策树可视化"><a href="#运用graphviz对决策树可视化" class="headerlink" title="运用graphviz对决策树可视化"></a>运用graphviz对决策树可视化</h1><p>决策树相关的知识我们已经有浅显的了解了（参见原来的博客<a href="https://xinzhuo777.github.io/2020/07/11/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/）">https://xinzhuo777.github.io/2020/07/11/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/）</a></p><p>由于决策过程我希望能直观化所以我研究了graphviz的一点点用法（更多用法见官网<a href="https://graphviz.org/gallery/）" target="_blank" rel="noopener">https://graphviz.org/gallery/）</a></p><p>Graphviz是一个开源的图（Graph）可视化软件，采用抽象的图和网络来表示结构化的信息。在数据科学领域，Graphviz的一个用途就是实现决策树可视化。</p><p>具体代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">train_data =pd.read_csv(<span class="string">'Tan-train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'Tan-test.csv'</span>)</span><br><span class="line"><span class="comment"># # 数据探索</span></span><br><span class="line"><span class="comment"># 使用平均年龄来填充年龄中的nan值</span></span><br><span class="line">train_data[<span class="string">'Age'</span>].fillna(train_data[<span class="string">'Age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Age'</span>].fillna(test_data[<span class="string">'Age'</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 使用票价的均值填充票价中的nan值</span></span><br><span class="line">train_data[<span class="string">'Fare'</span>].fillna(train_data[<span class="string">'Fare'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Fare'</span>].fillna(test_data[<span class="string">'Fare'</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 使用登录最多的港口来填充登录港口的nan值</span></span><br><span class="line">train_data[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line">features = [<span class="string">'Pclass'</span>, <span class="string">'Sex'</span>, <span class="string">'Age'</span>, <span class="string">'SibSp'</span>, <span class="string">'Parch'</span>, <span class="string">'Fare'</span>, <span class="string">'Embarked'</span>]</span><br><span class="line">train_features = train_data[features]</span><br><span class="line">train_labels = train_data[<span class="string">'Survived'</span>]</span><br><span class="line">test_features = test_data[features]</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">dvec=DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">train_features=dvec.fit_transform(train_features.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"><span class="comment"># print(dvec.feature_names_)</span></span><br><span class="line"><span class="comment"># print(dvec.feature_names_)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="comment"># 构造ID3决策树</span></span><br><span class="line">clf = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)</span><br><span class="line"><span class="comment"># 决策树训练</span></span><br><span class="line">clf.fit(train_features, train_labels)</span><br><span class="line">test_features=dvec.transform(test_features.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"><span class="comment"># 决策树预测</span></span><br><span class="line">pred_labels = clf.predict(test_features)</span><br><span class="line">test_answer=pd.read_csv(<span class="string">'gender_submission.csv'</span>)</span><br><span class="line">test_answer_array=np.array(test_answer[<span class="string">'Survived'</span>])</span><br><span class="line">new_array=test_answer_array+pred_labels</span><br><span class="line"><span class="keyword">from</span> collections  <span class="keyword">import</span> Counter</span><br><span class="line">end_for=Counter(new_array)</span><br><span class="line">print(end_for)</span><br><span class="line">acc_decision_tree = round(clf.score(train_features, train_labels), <span class="number">6</span>)</span><br><span class="line">print(<span class="string">u'score准确率为 %.4lf'</span> % acc_decision_tree)</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 使用K折交叉验证 统计决策树准确率</span></span><br><span class="line">print(<span class="string">u'cross_val_score准确率为 %.4lf'</span> % np.mean(cross_val_score(clf, train_features, train_labels, cv=<span class="number">10</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="literal">None</span>,</span><br><span class="line">                                feature_names=dvec.feature_names_, <span class="comment"># 特征名称</span></span><br><span class="line">                                class_names=[<span class="string">'No'</span>, <span class="string">'Yes'</span>], <span class="comment"># 目标变量的类别名</span></span><br><span class="line">                                filled=<span class="literal">True</span>, rounded=<span class="literal">True</span>,</span><br><span class="line">                                special_characters=<span class="literal">True</span>)</span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.render(<span class="string">'example.gv'</span>, directory=<span class="string">'E:\\log_reg'</span>, view=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">'Save example.gv file!\n'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Xinzhuo777/picgo/master/image0001.jpg"></p><p>重点是下面这段代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import tree</span><br><span class="line">import graphviz</span><br><span class="line">dot_data &#x3D; tree.export_graphviz(clf, out_file&#x3D;None,</span><br><span class="line">                                feature_names&#x3D;dvec.feature_names_, # 特征名称</span><br><span class="line">                                class_names&#x3D;[&#39;No&#39;, &#39;Yes&#39;], # 目标变量的类别名</span><br><span class="line">                                filled&#x3D;True, rounded&#x3D;True,</span><br><span class="line">                                special_characters&#x3D;True)</span><br><span class="line">graph &#x3D; graphviz.Source(dot_data)</span><br><span class="line">graph.render(&#39;example.gv&#39;, directory&#x3D;&#39;E:\\log_reg&#39;, view&#x3D;True)</span><br><span class="line">print(&#39;Save example.gv file!\n&#39;)</span><br></pre></td></tr></table></figure><p>我们首先要创立一个dot文件来描述决策树</p><p>tree.export_graphviz参数说明如下：</p><p>feature_names:特征名称，顺序必须和训练样本的数据必须一致</p><p>class_names:类别名称，输入的时候，必须要排序。如将原来的[‘1’, ‘0’]设置为[‘0’, ‘1’]，注意：数据类型必须为str型的</p><p>filled:填充，且必须为True</p><p>node_ids:节点id，且必须为True</p><p>rounded：画的图形边缘是否美化，必须为True</p><p>special_characters:必须为True</p><h4 id="代码原理"><a href="#代码原理" class="headerlink" title="代码原理"></a>代码原理</h4><p>通过一种反向搜索的方法遍历决策树结果，将结果转换成标准sql</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;运用graphviz对决策树可视化&quot;&gt;&lt;a href=&quot;#运用graphviz对决策树可视化&quot; class=&quot;headerlink&quot; title=&quot;运用graphviz对决策树可视化&quot;&gt;&lt;/a&gt;运用graphviz对决策树可视化&lt;/h1&gt;&lt;p&gt;决策树相关的知识我们</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>[SVM]</title>
    <link href="http://xinzhuo777.github.io/2020/08/01/SVM/"/>
    <id>http://xinzhuo777.github.io/2020/08/01/SVM/</id>
    <published>2020-08-01T05:15:38.000Z</published>
    <updated>2020-09-10T11:08:30.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SVM算法原理-一"><a href="#SVM算法原理-一" class="headerlink" title="SVM算法原理(一)"></a>SVM算法原理(一)</h1><h6 id="间隔"><a href="#间隔" class="headerlink" title="间隔"></a>间隔</h6><p>在给定训练集</p><script type="math/tex; mode=display">D=[(x_1,y_1）,(x_2，y_2),....,(x_m,y_m)]</script><script type="math/tex; mode=display">y_i=+1or-1</script><ul><li>函数间隔和几何间隔</li></ul><p>我们定义一个超平面的表达式为</p><script type="math/tex; mode=display">\pmb{y}=w^T\pmb{x}+b</script><p>则超平面上的一点x到该平面的距离(几何间隔)为：</p><script type="math/tex; mode=display">\frac{|w^Tx+b|}{||w||}</script><p>函数间隔考虑到y取值仅有1和-1，对于正样例则label为1，反之为-1，即乘不改变其绝对值，故函数间隔的定义如下：</p><script type="math/tex; mode=display">y_i(w^Tx+b)</script><p>来表示样本点到这个平面的函数间隔</p><ul><li><p>函数间隔和几何间隔的关系</p><p>如果找到了一个超平面可以正确分开这个数据集，那么意味着它的函数间隔大于和等于0，即对于一个正样例$y_i$=1,因为被正确分类了，所以$w^Tx+b&gt;0$,则有$y_i(w^T+b)&gt;0$(实际上有更严格限制的条件)，负样例同理。</p><p>由于$|y_i|=1$,所以函数间隔可以用几何间隔表示，即在样本集线性可分的情况下，对于一个成功分类的超平面而言，每个样本点到这个超平面的几何间隔就是函数间隔除以$w$范数</p></li><li><p>定义超平面到训练集的间隔</p></li></ul><p>我们定义超平面到训练集的间隔为这个超平面到训练集每个点的几何距离中的最小距离</p><p>$min\frac{y_i(w^Tx_i+b)}{||w||}$,表示为在每个几何间隔中找到最小的。因为$\frac{1}{||w||}$和具体的样本点无关，所以提出来原式化为$\frac{1}{||w||}min(y_i(w^Tx_i+b))$,表示为训练集每一个点到这个超平面的函数间隔中的最小值。</p><h5 id="SVM形式"><a href="#SVM形式" class="headerlink" title="SVM形式"></a>SVM形式</h5><p>基于以上的定义我们自然而然想到，我们要找到一个超平面，它不但可以正确分类，而且它到训练集的间隔是所有可以正确分类的超平面中最大的。即找到一组（w，b）使得$max(\frac{1}{||w||}min(y_i(w^Tx_i+b)))$</p><p><strong>我们需要注意到一个点</strong>：对于w和b同时进行缩放，即w-&gt;kw,b-&gt;kb,带入原式，值不变。</p><p>所以我们通过合理放缩使得$min(y_i(w^Tx_i+b))$=1(变相增加了约束)</p><p>对于不是离超平面最近的那些样本点，它们的$y_i(w^Tx_i+b)&gt;1$，所以化简完，即求解$max\frac{1}{||w||}$再次等价为$\frac{1}{2}min||w||^2$,$\frac{1}{2}$为了后面推导方便。</p><h4 id="构造拉格朗日函数"><a href="#构造拉格朗日函数" class="headerlink" title="构造拉格朗日函数"></a>构造拉格朗日函数</h4><p>我们总结一下经过转化后条件有两个：</p><script type="math/tex; mode=display">\frac{1}{2}min\frac{1}{||w||^2}</script><script type="math/tex; mode=display">y_i(w^Tx_i+b)\ge1</script><p>构造拉格朗日函数</p><script type="math/tex; mode=display">L(w,b,\alpha)=\frac{1}{2}*\frac{1}{||w||}-\sum_{1}^{n}\alpha_i*(y_i*(w^T*x_i+b)-1)</script><p>则原优化问题可以转化为</p><p>$\theta(w)=maxL(w,b,\alpha),\alpha_i&gt;0$</p><p>求偏导得到： </p><script type="math/tex; mode=display">\pmb{w}=\sum_{i=1}^{n}\alpha_iy_i\pmb{x_i}</script><script type="math/tex; mode=display">0=\sum_{i=1}^{n}\alpha_iy_i</script><p>代入有</p><script type="math/tex; mode=display">max(\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j\pmb{x_i^{T}x_j})</script><p>如何求解上述式子，运用KKT条件</p><script type="math/tex; mode=display">\left\{ \begin{array}{rcl} \alpha_i>0  & & {    }\\ y_i(w^T\pmb{x}+b)\ge0    &      & {}\\ \alpha_i(y_i(w^T\pmb{x}+b)-1) =0 {}\end{array}\right.</script><p>这是一个对偶问题（凸优化，对偶问题，具体理论太数学，目前水平有限就暂时不做自己推导了）</p><h3 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h3><p>整个SMO算法包括两部分，求解两个变量的二次规划问题和选择这两个变量的启发式方法。我觉得SMO算法难就难在对两个α变量的选择过程。</p><p>$\sum_{i=1}^{n}\alpha_iy_i=0$,如果取$\alpha_1和\alpha_2$,定下$\alpha_2$的值，利用KKT条件，我们可以得出$\alpha_1$的值</p><p>这是一个关于两个$\alpha$的二次规划问题（我不想手敲了。。。）</p><h2 id="论文资料"><a href="#论文资料" class="headerlink" title="论文资料"></a>论文资料</h2><p>《Sequential Minimal Optimization A Fast Algorithm for Training Support Vector Machines》</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>模仿sklearn库自带的smo函数实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, C=<span class="number">1</span>, toler=<span class="number">0.001</span>, maxIter=<span class="number">500</span>, kernel_option=<span class="params">(<span class="string">""</span>, <span class="number">0</span>)</span>)</span>:</span></span><br><span class="line">        self.C = C  <span class="comment"># 惩罚参数</span></span><br><span class="line">        self.toler = toler  <span class="comment"># 迭代的终止条件之一</span></span><br><span class="line">        self.b = <span class="number">0</span>  <span class="comment"># 阈值</span></span><br><span class="line">        self.max_iter = maxIter  <span class="comment"># 最大迭代次数</span></span><br><span class="line">        self.kernel_opt = kernel_option  <span class="comment"># 选用的核函数及其参数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SVM_training</span><span class="params">(self, dataSet, labels ,)</span>:</span></span><br><span class="line">        <span class="comment"># 1.输入数据集</span></span><br><span class="line">        <span class="comment"># train_x_m, train_y_m = np.mat(train_x), np.mat(train_y)dataSet, labels,</span></span><br><span class="line">        self.train_x = np.mat(dataSet)  <span class="comment"># 训练数据集</span></span><br><span class="line">        self.train_y = np.mat(labels)  <span class="comment"># 测试数据集</span></span><br><span class="line">        self.train_y = self.train_y.T <span class="keyword">if</span> np.shape(self.train_y)[<span class="number">0</span>] == <span class="number">1</span> <span class="keyword">else</span> self.train_y  <span class="comment"># 将其转化为列向量</span></span><br><span class="line">        self.n_samples = np.shape(dataSet)[<span class="number">0</span>]  <span class="comment"># 训练样本的个数</span></span><br><span class="line">        self.alphas = np.mat(np.zeros((self.n_samples, <span class="number">1</span>)))  <span class="comment"># 拉格朗日乘子（一个全0的列向量）</span></span><br><span class="line">        self.error_tmp = np.mat(np.zeros((self.n_samples, <span class="number">2</span>)))  <span class="comment"># 保存E的缓存</span></span><br><span class="line">        self.kernel_mat = self.calc_kernel(self.train_x, self.kernel_opt)  <span class="comment"># 核函数的输出</span></span><br><span class="line">        <span class="comment"># 2.开始训练</span></span><br><span class="line">        entireSet = <span class="literal">True</span></span><br><span class="line">        alpha_pairs_changed = <span class="number">0</span></span><br><span class="line">        iteration = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> iteration &lt; self.max_iter <span class="keyword">and</span> (alpha_pairs_changed &gt; <span class="number">0</span> <span class="keyword">or</span> entireSet):</span><br><span class="line">            print(<span class="string">"\t iteration: "</span>, iteration)</span><br><span class="line">            alpha_pairs_changed = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> entireSet:  <span class="comment"># 对所有样本</span></span><br><span class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> range(self.n_samples):</span><br><span class="line">                    alpha_pairs_changed += self.choose_and_update(x)</span><br><span class="line">                iteration += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 对非边界样本</span></span><br><span class="line">                bound_samples = []</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_samples):</span><br><span class="line">                    <span class="keyword">if</span> self.alphas[i, <span class="number">0</span>] &gt; <span class="number">0</span> <span class="keyword">and</span> self.alphas[i, <span class="number">0</span>] &lt; self.C:</span><br><span class="line">                        bound_samples.append(i)</span><br><span class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> bound_samples:</span><br><span class="line">                    alpha_pairs_changed += self.choose_and_update(x)</span><br><span class="line">                iteration += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> entireSet:</span><br><span class="line">                entireSet = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">elif</span> alpha_pairs_changed == <span class="number">0</span>:</span><br><span class="line">                entireSet = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_error</span><span class="params">(self, alpha_index_k)</span>:</span></span><br><span class="line">        <span class="comment">#误差值的计算</span></span><br><span class="line">        predict_k = float(np.multiply(self.alphas, self.train_y).T * self.kernel_mat[:, alpha_index_k] + self.b)</span><br><span class="line">        error_k = predict_k - float(self.train_y[alpha_index_k])</span><br><span class="line">        <span class="keyword">return</span> error_k</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_second_sample_j</span><span class="params">(self, alpha_index_i, error_i)</span>:</span></span><br><span class="line">        <span class="comment">#选择第二个变量</span></span><br><span class="line">        <span class="comment"># :param alpha_index_i(float): 第一个变量alpha_i的index_i</span></span><br><span class="line">        <span class="comment"># :param error_i(float): E_i</span></span><br><span class="line">        <span class="comment"># :return:第二个变量alpha_j的index_j和误差值E_j</span></span><br><span class="line"></span><br><span class="line">        self.error_tmp[alpha_index_i] = [<span class="number">1</span>, error_i]  <span class="comment"># 用来标记已被优化</span></span><br><span class="line">        candidate_alpha_list = np.nonzero(self.error_tmp[:, <span class="number">0</span>].A)[<span class="number">0</span>]  <span class="comment"># 因为是列向量，列数[1]都为0，只需记录行数[0]</span></span><br><span class="line">        max_step, max_step, error_j = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(candidate_alpha_list) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">for</span> alpha_index_k <span class="keyword">in</span> candidate_alpha_list:</span><br><span class="line">                <span class="keyword">if</span> alpha_index_k == alpha_index_i:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                error_k = self.cal_error(alpha_index_k)</span><br><span class="line">                <span class="keyword">if</span> abs(error_k - error_i) &gt; max_step:</span><br><span class="line">                    max_step = abs(error_k - error_i)</span><br><span class="line">                    alpha_index_j, error_j = alpha_index_k, error_k</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 随机选择</span></span><br><span class="line">            alpha_index_j = alpha_index_i</span><br><span class="line">            <span class="keyword">while</span> alpha_index_j == alpha_index_i:</span><br><span class="line">                alpha_index_j = np.random.randint(<span class="number">0</span>, self.n_samples)</span><br><span class="line">            error_j = self.cal_error(alpha_index_j)</span><br><span class="line">        <span class="keyword">return</span> alpha_index_j, error_j</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_error_tmp</span><span class="params">(self, alpha_index_k)</span>:</span></span><br><span class="line">        <span class="comment"># 重新计算误差值，并对其标记为已被优化</span></span><br><span class="line">        <span class="comment"># :param alpha_index_k: 要计算的变量α</span></span><br><span class="line">        <span class="comment"># :return: index为k的alpha新的误差</span></span><br><span class="line">        error = self.cal_error(alpha_index_k)</span><br><span class="line">        self.error_tmp[alpha_index_k] = [<span class="number">1</span>, error]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_and_update</span><span class="params">(self, alpha_index_i)</span>:</span></span><br><span class="line">        <span class="comment"># 判断和选择两个alpha</span></span><br><span class="line">        error_i = self.cal_error(alpha_index_i)  <span class="comment"># 计算第一个样本的E_i</span></span><br><span class="line">        <span class="keyword">if</span> (self.train_y[alpha_index_i] * error_i &lt; -self.toler) <span class="keyword">and</span> (self.alphas[alpha_index_i] &lt; self.C) \</span><br><span class="line">                <span class="keyword">or</span> (self.train_y[alpha_index_i] * error_i &gt; self.toler) <span class="keyword">and</span> (self.alphas[alpha_index_i] &gt; <span class="number">0</span>):</span><br><span class="line">            <span class="comment"># 1.选择第二个变量</span></span><br><span class="line">            alpha_index_j, error_j = self.select_second_sample_j(alpha_index_i, error_i)</span><br><span class="line">            alpha_i_old = self.alphas[alpha_index_i].copy()</span><br><span class="line">            alpha_j_old = self.alphas[alpha_index_j].copy()</span><br><span class="line">            <span class="comment"># 2.计算上下界</span></span><br><span class="line">            <span class="keyword">if</span> self.train_y[alpha_index_i] != self.train_y[alpha_index_j]:</span><br><span class="line">                L = max(<span class="number">0</span>, self.alphas[alpha_index_j] - self.alphas[alpha_index_i])</span><br><span class="line">                H = min(self.C, self.C + self.alphas[alpha_index_j] - self.alphas[alpha_index_i])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                L = max(<span class="number">0</span>, self.alphas[alpha_index_j] + self.alphas[alpha_index_i] - self.C)</span><br><span class="line">                H = min(self.C, self.alphas[alpha_index_j] + self.alphas[alpha_index_i])</span><br><span class="line">            <span class="keyword">if</span> L == H:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            <span class="comment"># 3.计算eta</span></span><br><span class="line">            eta = self.kernel_mat[alpha_index_i, alpha_index_i] + self.kernel_mat[alpha_index_j, alpha_index_j] - <span class="number">2.0</span> * \</span><br><span class="line">                  self.kernel_mat[alpha_index_i, alpha_index_j]</span><br><span class="line">            <span class="keyword">if</span> eta &lt;= <span class="number">0</span>:  <span class="comment"># 因为这个eta&gt;=0</span></span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            <span class="comment"># 4.更新alpha_j</span></span><br><span class="line">            self.alphas[alpha_index_j] += self.train_y[alpha_index_j] * (error_i - error_j) / eta</span><br><span class="line">            <span class="comment"># 5.根据范围确实最终的j</span></span><br><span class="line">            <span class="keyword">if</span> self.alphas[alpha_index_j] &gt; H:</span><br><span class="line">                self.alphas[alpha_index_j] = H</span><br><span class="line">            <span class="keyword">if</span> self.alphas[alpha_index_j] &lt; L:</span><br><span class="line">                self.alphas[alpha_index_j] = L</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 6.判断是否结束</span></span><br><span class="line">            <span class="keyword">if</span> abs(alpha_j_old - self.alphas[alpha_index_j]) &lt; <span class="number">0.00001</span>:</span><br><span class="line">                self.update_error_tmp(alpha_index_j)</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            <span class="comment"># 7.更新alpha_i</span></span><br><span class="line">            self.alphas[alpha_index_i] += self.train_y[alpha_index_i] * self.train_y[alpha_index_j] * (</span><br><span class="line">                        alpha_j_old - self.alphas[alpha_index_j])</span><br><span class="line">            <span class="comment"># 8.更新b</span></span><br><span class="line">            b1 = self.b - error_i - self.train_y[alpha_index_i] * self.kernel_mat[alpha_index_i, alpha_index_i] * (</span><br><span class="line">                        self.alphas[alpha_index_i] - alpha_i_old) \</span><br><span class="line">                 - self.train_y[alpha_index_j] * self.kernel_mat[alpha_index_i, alpha_index_j] * (</span><br><span class="line">                             self.alphas[alpha_index_j] - alpha_j_old)</span><br><span class="line">            b2 = self.b - error_j - self.train_y[alpha_index_i] * self.kernel_mat[alpha_index_i, alpha_index_j] * (</span><br><span class="line">                        self.alphas[alpha_index_i] - alpha_i_old) \</span><br><span class="line">                 - self.train_y[alpha_index_j] * self.kernel_mat[alpha_index_j, alpha_index_j] * (</span><br><span class="line">                             self.alphas[alpha_index_j] - alpha_j_old)</span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt; self.alphas[alpha_index_i] <span class="keyword">and</span> self.alphas[alpha_index_i] &lt; self.C:</span><br><span class="line">                self.b = b1</span><br><span class="line">            <span class="keyword">elif</span> <span class="number">0</span> &lt; self.alphas[alpha_index_j] <span class="keyword">and</span> self.alphas[alpha_index_j] &lt; self.C:</span><br><span class="line">                self.b = b2</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.b = (b1 + b2) / <span class="number">2.0</span></span><br><span class="line">            <span class="comment"># 9.更新error</span></span><br><span class="line">            self.update_error_tmp(alpha_index_j)</span><br><span class="line">            self.update_error_tmp(alpha_index_i)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">svm_predict</span><span class="params">(self, test_data_x)</span>:</span></span><br><span class="line">        <span class="comment">#return预测值</span></span><br><span class="line">        kernel_value = self.calc_kernel_value(self.train_x, test_data_x, self.kernel_opt)</span><br><span class="line">        alp = self.alphas</span><br><span class="line">        predict = np.multiply(self.train_y, self.alphas).T * kernel_value + self.b</span><br><span class="line">        <span class="keyword">return</span> predict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_prediction</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        <span class="comment"># 对样本进行预测（预测多个数据）</span></span><br><span class="line">        <span class="comment"># input:  test_data(mat):测试数据</span></span><br><span class="line">        <span class="comment"># output: prediction(list):预测所属的类别</span></span><br><span class="line">        m = np.shape(test_data)[<span class="number">0</span>]</span><br><span class="line">        prediction = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            predict = self.svm_predict(test_data[i, :])</span><br><span class="line">            prediction.append(str(np.sign(predict)[<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> prediction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_accuracy</span><span class="params">(self, test_x, test_y)</span>:</span></span><br><span class="line">        <span class="comment"># 计算准确率</span></span><br><span class="line">        n_samples = np.shape(test_x)[<span class="number">0</span>]</span><br><span class="line">        correct = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">            predict = self.svm_predict(test_x[i, :])</span><br><span class="line">            <span class="keyword">if</span> np.sign(predict) == np.sign(test_y[i]):</span><br><span class="line">                correct += <span class="number">1</span></span><br><span class="line">        accuracy = correct / n_samples</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_train_accracy</span><span class="params">(self)</span>:</span></span><br><span class="line">        accuracy = self.cal_accuracy(self.train_x, self.train_y)</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_kernel</span><span class="params">(self, train_x, kernel_option)</span>:</span></span><br><span class="line">        <span class="comment"># 计算核函数的矩阵</span></span><br><span class="line">        <span class="comment"># :param train_x(matrix): 训练样本的特征值</span></span><br><span class="line">        <span class="comment"># :param kernel_option(tuple):  核函数的类型以及参数</span></span><br><span class="line">        <span class="comment"># :return: kernel_matrix(matrix):  样本的核函数的值</span></span><br><span class="line">        m = np.shape(train_x)[<span class="number">0</span>]</span><br><span class="line">        kernel_matrix = np.mat(np.zeros((m, m)))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            kernel_matrix[:, i] = self.calc_kernel_value(train_x, train_x[i, :], kernel_option)</span><br><span class="line">        <span class="keyword">return</span> kernel_matrix</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_kernel_value</span><span class="params">(self, train_x, train_x_i, kernel_option)</span>:</span></span><br><span class="line">        <span class="comment"># 样本之间的核函数值</span></span><br><span class="line">        <span class="comment"># :param train_x(matrix): 训练样本</span></span><br><span class="line">        <span class="comment"># :param train_x_i(matrix):   第i个训练样本 一个行向量</span></span><br><span class="line">        <span class="comment"># :param kernel_option(tuple):   核函数的类型以及参数</span></span><br><span class="line">        <span class="comment"># :return: kernel_value(matrix):  样本之间的核函数值</span></span><br><span class="line">        kernel_type = kernel_option[<span class="number">0</span>]</span><br><span class="line">        m = np.shape(train_x)[<span class="number">0</span>]</span><br><span class="line">        kernel_value = np.mat(np.zeros((m, <span class="number">1</span>)))</span><br><span class="line">        <span class="keyword">if</span> kernel_type == <span class="string">"rbf"</span>:  <span class="comment"># 高斯核函数</span></span><br><span class="line">            sigma = kernel_option[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> sigma == <span class="number">0</span>:</span><br><span class="line">                sigma = <span class="number">1.0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">                diff = train_x[i, :] - train_x_i</span><br><span class="line">                kernel_value[i] = np.exp(diff * diff.T / (<span class="number">-2.0</span> * sigma ** <span class="number">2</span>))  <span class="comment"># 分子为差的2范数的平方</span></span><br><span class="line">        <span class="keyword">elif</span> kernel_type == <span class="string">"polynomial"</span>:</span><br><span class="line">            p = kernel_option[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">                kernel_value[i] = (train_x[i, :] * train_x_i.T + <span class="number">1</span>) ** p</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            kernel_value = train_x * train_x_i.T  <span class="comment"># 直接一个m*m矩阵×一个m*1的矩阵</span></span><br><span class="line">        <span class="keyword">return</span> kernel_value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_prediction</span><span class="params">(self, result_file, prediction)</span>:</span></span><br><span class="line">        <span class="comment"># 保存预测的结果</span></span><br><span class="line">        <span class="comment"># input:  result_file(string):结果保存的文件</span></span><br><span class="line">        <span class="comment">#         prediction(list):预测的结果</span></span><br><span class="line">        f = open(result_file, <span class="string">'w'</span>)</span><br><span class="line">        f.write(<span class="string">" "</span>.join(prediction))</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(data_file)</span>:</span></span><br><span class="line">    data_set, labels = [], []</span><br><span class="line">    <span class="keyword">with</span> open(data_file, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        textlist = f.readlines()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> textlist:</span><br><span class="line">            tmp = []</span><br><span class="line">            line = line.strip().split(<span class="string">" "</span>)</span><br><span class="line">            line_0=[]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> line[<span class="number">0</span>].split(<span class="string">','</span>):</span><br><span class="line">                line_0.append(float(i))</span><br><span class="line">            labels.append(line_0[<span class="number">8</span>])</span><br><span class="line">            i = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> line_0[<span class="number">0</span>:<span class="number">8</span>]:</span><br><span class="line"></span><br><span class="line">                tmp.append(word)</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            data_set.append(tmp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (np.mat(data_set), np.mat(labels).T)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    filepath=<span class="string">'D:\\Python code\\machine learning\\pima-indians-diabetes.csv'</span></span><br><span class="line">    train_x, train_y = load_data(filepath)</span><br><span class="line">    <span class="comment"># print(train_y,train_x)</span></span><br><span class="line">    svm = SVM(C=<span class="number">0.6</span>, kernel_option=(<span class="string">"rbf"</span>, <span class="number">0.43</span>))</span><br><span class="line">    svm = svm.SVM_training(train_x, train_y,)</span><br><span class="line">    <span class="comment"># print(svm.alphas,svm.b)</span></span><br><span class="line">    accuracy = svm.get_train_accracy()</span><br><span class="line">    print(<span class="string">"The training accuracy is: %.3f%%"</span> % (accuracy * <span class="number">100</span>))</span><br></pre></td></tr></table></figure><p>样例<br><img src="https://raw.githubusercontent.com/Xinzhuo777/picgo/master/imagesvm%20.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SVM算法原理-一&quot;&gt;&lt;a href=&quot;#SVM算法原理-一&quot; class=&quot;headerlink&quot; title=&quot;SVM算法原理(一)&quot;&gt;&lt;/a&gt;SVM算法原理(一)&lt;/h1&gt;&lt;h6 id=&quot;间隔&quot;&gt;&lt;a href=&quot;#间隔&quot; class=&quot;headerlink</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>PCA算法原理及实现</title>
    <link href="http://xinzhuo777.github.io/2020/07/20/PCA/"/>
    <id>http://xinzhuo777.github.io/2020/07/20/PCA/</id>
    <published>2020-07-20T08:01:19.000Z</published>
    <updated>2020-08-01T05:07:06.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PCA算法原理及实现"><a href="#PCA算法原理及实现" class="headerlink" title="PCA算法原理及实现"></a>PCA算法原理及实现</h1><p>PCA算法是一种机器学习常见的数据降维方法</p><h2 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h2><p>PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。</p><h2 id="PCA算法的两种实现方法"><a href="#PCA算法的两种实现方法" class="headerlink" title="PCA算法的两种实现方法"></a>PCA算法的两种实现方法</h2><p>（1）基于特征值分解协方差矩阵</p><p>​        1：去中心化，即每一位特征减去各自的平均值</p><p>​        2：计算协方差矩阵$\frac{1}{n}XX^{T}$</p><p>​        3：用特征值分解求协方差矩阵$\frac{1}{n}XX^{T}$的特征值和特征向量</p><p>​        4：对特征值从大到小排序，选择最大的前k个，将其对应的k个特征向量组成特征向量的矩阵P</p><p>​        5：将数据转换到k个特征向量组成的新的空间，即Y=PX</p><p>（2）基于SVD分解协方差矩阵实现PCA算法</p><p>​        输入一个n维的数据集，将其降维k维</p><p>​        1:去平均值，即每一位的特征值减去平均值</p><p>​        2：计算协方差矩阵</p><p>​        3：通过SVD计算协方差矩阵的特征值与特征向量</p><p>​        4：对特征值从大到小排序，选择最大的前k个，将其对应的k个特征向量组成特征向量的矩阵P</p><p>​        5：将数据转换到k个特征向量组成的新的空间，即Y=PX</p><p>​        （scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是特征值分解。）</p><h2 id="代码实现：（基于自己写的代码和sklearn库自带的pca算法对比）"><a href="#代码实现：（基于自己写的代码和sklearn库自带的pca算法对比）" class="headerlink" title="代码实现：（基于自己写的代码和sklearn库自带的pca算法对比）"></a>代码实现：（基于自己写的代码和sklearn库自带的pca算法对比）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loaddataset</span><span class="params">(filename)</span>:</span></span><br><span class="line">    df=pd.read_table(filename,sep=<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">return</span> np.array(df)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">showdata</span><span class="params">(datamat,reconmat)</span>:</span></span><br><span class="line">    fig=plt.figure()</span><br><span class="line">    ax=fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.scatter(datamat[:,<span class="number">0</span>],datamat[:,<span class="number">1</span>],c=<span class="string">'green'</span>)</span><br><span class="line">    ax.scatter(np.array(reconmat[:,<span class="number">0</span>]),reconmat[:,<span class="number">0</span>],c=<span class="string">'red'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(datamat,topnfeat=<span class="number">9</span>)</span>:</span></span><br><span class="line">    <span class="comment">#对所有样本去中心化</span></span><br><span class="line">    meanvals=np.mean(datamat,axis=<span class="number">0</span>)</span><br><span class="line">    meanremoved=datamat-meanvals</span><br><span class="line">    <span class="comment">#计算样本的协方差矩阵</span></span><br><span class="line">    covmat=np.cov(meanremoved,rowvar=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#print(covmat)</span></span><br><span class="line">    <span class="comment">#对协方差矩阵做特征值分解，求得其特征值和特征向量，并将特征值从大到小排序</span></span><br><span class="line">    eigvals,eigvects=np.linalg.eig(np.mat(covmat))</span><br><span class="line">    eigvalind=np.argsort(eigvals)</span><br><span class="line">    eigvalind=eigvalind[:-(topnfeat+<span class="number">1</span>):<span class="number">-1</span>]</span><br><span class="line">    redeigvects=eigvects[:,eigvalind]</span><br><span class="line">    <span class="comment">#降维</span></span><br><span class="line">    lowdatamat=meanremoved*redeigvects</span><br><span class="line">    reconmat=(lowdatamat*redeigvects.T)+meanvals</span><br><span class="line">    <span class="keyword">return</span> np.array(lowdatamat),np.array(reconmat)</span><br><span class="line"></span><br><span class="line">datamat=loaddataset(<span class="string">'testSet.txt'</span>)</span><br><span class="line">lowdatamat,reconmat=pca(datamat,<span class="number">1</span>)</span><br><span class="line">showdata(datamat,reconmat)</span><br><span class="line">print(<span class="string">'\n test for sklearn \n'</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca2=PCA(n_components=<span class="number">1</span>)</span><br><span class="line">datamatnew=datamat</span><br><span class="line">pca2.fit(datamatnew)</span><br><span class="line">g=pca2.transform(datamatnew)</span><br><span class="line">print(g)</span><br><span class="line">showdata(datamat,g)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line">print(lowdatamat)</span><br></pre></td></tr></table></figure><p>代码结果对比</p><p><img src="/2020/07/20/PCA/my own pca.png" alt="my own pca"></p><p>这是个人写的pca算法</p><p><img src="/2020/07/20/PCA/sklearn pca.png" alt="sklearn pca"></p><p>sklearn自带的pca算法</p><p>从图像的结果可知，两种算法给出的降维结果其实是一致的，区别的点，是由于本身算法给出的结果的缩放导致</p><h4 id="基于pca算法的直观理解"><a href="#基于pca算法的直观理解" class="headerlink" title="基于pca算法的直观理解"></a>基于pca算法的直观理解</h4><p>n维投影到k维，我们所选择的投影方式使得在每一维上尽可能使得样本的方差最大。详细的数学方面的请自行搜索最大方差理论，当然还有更多数学上的理论解释。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;PCA算法原理及实现&quot;&gt;&lt;a href=&quot;#PCA算法原理及实现&quot; class=&quot;headerlink&quot; title=&quot;PCA算法原理及实现&quot;&gt;&lt;/a&gt;PCA算法原理及实现&lt;/h1&gt;&lt;p&gt;PCA算法是一种机器学习常见的数据降维方法&lt;/p&gt;
&lt;h2 id=&quot;主要思想</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>CART算法</title>
    <link href="http://xinzhuo777.github.io/2020/07/13/CART%20Decision%20Tree/"/>
    <id>http://xinzhuo777.github.io/2020/07/13/CART%20Decision%20Tree/</id>
    <published>2020-07-12T16:00:06.000Z</published>
    <updated>2020-07-13T10:47:58.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h1><p>决策树算法先前部分<a href="https://xinzhuo777.github.io/2020/07/11/决策树原理学习笔记/">https://xinzhuo777.github.io/2020/07/11/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a></p><p> CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。（关于分类和回归的区别，Andrew Ng的Machine Learning课程给出的定义：Supervised learning problems are categorized into “regression” and “classification” problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in adiscrete output. In other words, we are trying to map input variables into discrete categories.）所以大体上，分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。</p><h2 id="CART-分类树的工作流程"><a href="#CART-分类树的工作流程" class="headerlink" title="CART 分类树的工作流程"></a>CART 分类树的工作流程</h2><p>与前面两种算法不同，CART算法的属性选择的指标采用的是基尼系数。假设 t 为节点，那么该节点的 GINI 系数的计算公式为：</p><script type="math/tex; mode=display">GINI(t)=1-\sum_k{[p(C_k|t)]^2}</script><p>这里 p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。</p><p>在属性 A 的划分下，节点 D 的基尼系数为：</p><script type="math/tex; mode=display">GINI(D,A)=\frac{D_1}{D}GINI(D_1)+\frac{D_2}{D}GINI(D_2)</script><p>（统计学习方法）例题：计算下面基尼系数</p><p><img src="\images\ca2.jpg" alt="ca2"></p><p><img src="\images\ca1.jpg" alt="ca1"></p><p><img src="/images/ca3.jpg" alt="ca3"></p><p>从这个题中我们可以发现CART算法只是以基尼系数选择最优特征，来决定该特征的最优二值切分点</p><h2 id="结合Sklearn库的CART算法实现"><a href="#结合Sklearn库的CART算法实现" class="headerlink" title="结合Sklearn库的CART算法实现"></a>结合Sklearn库的CART算法实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">iris=load_iris()</span><br><span class="line"><span class="comment"># 获取特征集和分类标识</span></span><br><span class="line">features = iris.data</span><br><span class="line">labels = iris.target</span><br><span class="line"><span class="comment"># 随机抽取70%的数据作为测试集，其余为训练集</span></span><br><span class="line">train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=<span class="number">0.70</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 创建CART分类树</span></span><br><span class="line">clf = DecisionTreeClassifier(criterion=<span class="string">'gini'</span>)</span><br><span class="line"><span class="comment"># 拟合构造CART分类树</span></span><br><span class="line">clf = clf.fit(train_features, train_labels)</span><br><span class="line"><span class="comment"># 用CART分类树做预测</span></span><br><span class="line">test_predict = clf.predict(test_features)</span><br><span class="line"><span class="comment"># 预测结果与测试集结果作比对</span></span><br><span class="line">score = accuracy_score(test_labels, test_predict)</span><br><span class="line">print(<span class="string">"CART分类树准确率 %.2lf"</span> % score)</span><br></pre></td></tr></table></figure><p>Python 的 sklearn 中，如果想要创建 CART 分类树，可以直接使用 DecisionTreeClassifier 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。</p><p> clf = DecisionTreeClassifier(criterion=‘gini’) 初始化一棵 CART 分类树。这样你就可以对 CART 分类树进行训练。</p><p>clf.fit(train_features, train_labels) 函数，将训练集的特征值和分类标识作为参数进行拟合，得到 CART 分类树。</p><p> clf.predict(test_features) 函数进行预测，传入测试集的特征值，可以得到测试结果 test_predict。</p><p> accuracy_score(test_labels, test_predict) 函数，传入测试集的预测结果与实际的结果作为参数，得到准确率 score。</p><p>最后比较重要是train_test_split是sklearn.model_selection中的分离器函数，用于将数组或矩阵划分为训练集和测试集，函数样式为：<br>X_train, X_test, y_train, y_test = train_test_split(train_data, train_target, test_size, random_state，shuffle)</p><p>train_data：待划分的样本数据<br>train_target：待划分的对应样本数据的样本标签<br>test_size：1）浮点数，在0 ~ 1之间，表示样本占比（test_size = 0.3，则样本数据中有30%的数据作为测试数据，记入X_test，其余70%数据记入X_train，同时适用于样本标签）；2）整数，表示样本数据中有多少数据记入X_test中，其余数据记入X_train<br>random_state：随机数种子，种子不同，每次采的样本不一样；种子相同，采的样本不变（random_state不取，采样数据不同，但random_state等于某个值，采样数据相同，取0的时候也相同，这可以自己编程尝试下，不过想改变数值也可以设置random_state = int(time.time())）<br>shuffle：洗牌模式，1）shuffle = False，不打乱样本数据顺序；2）shuffle = True，打乱样本数据顺序<br>原文链接：<a href="https://blog.csdn.net/zhuqiang9607/article/details/83686308" target="_blank" rel="noopener">https://blog.csdn.net/zhuqiang9607/article/details/83686308</a></p><h2 id="CART回归树流程"><a href="#CART回归树流程" class="headerlink" title="CART回归树流程"></a>CART回归树流程</h2><p>样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设 x 为样本的个体，均值为 u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。</p><p>这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些</p><h2 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h2><p>（选自李航《统计学习方法》）</p><p><img src="\images\ca4.jpg" alt="ca4"><br><img src="\images\ca5.jpg" alt="ca5"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CART算法&quot;&gt;&lt;a href=&quot;#CART算法&quot; class=&quot;headerlink&quot; title=&quot;CART算法&quot;&gt;&lt;/a&gt;CART算法&lt;/h1&gt;&lt;p&gt;决策树算法先前部分&lt;a href=&quot;https://xinzhuo777.github.io/2020/07</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>K-means聚类算法</title>
    <link href="http://xinzhuo777.github.io/2020/07/12/K-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://xinzhuo777.github.io/2020/07/12/K-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</id>
    <published>2020-07-12T09:43:31.000Z</published>
    <updated>2020-07-12T14:10:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K-means的基本原理"><a href="#K-means的基本原理" class="headerlink" title="K-means的基本原理"></a>K-means的基本原理</h1><p>K-Means 是一种非监督学习，解决的是聚类问题。K 代表的是 K 类，Means 代表的是中心，这个算法的本质是确定 K 类的中心点，当你找到了这些中心点，也就完成了聚类。基本的步骤如下：（采用欧氏距离分析）</p><p>1.选取 K 个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的</p><p>2.将每个点分配到最近的类中心点，这样就形成了 K 个类，然后重新计算每个类的中心点</p><p>3.重复第二步，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束</p><h2 id="运用sklearn完成算法"><a href="#运用sklearn完成算法" class="headerlink" title="运用sklearn完成算法"></a>运用sklearn完成算法</h2><p>sklearn是Python中的库，内含K-means算法的函数，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">KMeans(n_clusters=<span class="number">8</span>, init=<span class="string">'k-means++'</span>, n_init=<span class="number">10</span>, max_iter=<span class="number">300</span>, tol=<span class="number">0.0001</span>, precompute_distances=<span class="string">'auto'</span>, verbose=<span class="number">0</span>, random_state=<span class="literal">None</span>, copy_x=<span class="literal">True</span>, n_jobs=<span class="number">1</span>, algorithm=<span class="string">'auto'</span>)</span><br></pre></td></tr></table></figure><p>n_clusters: 即 K 值，K值的选择会对结果产生重大影响。K值较小，相当于用较小的邻域中对于训练实例进行预测，“学习”的近似误差会减小，只有对于与输入实例相近的训练实例才会对预测起作用，但是缺点在于“学习”的估计误差被放大，对于近邻的实例点非常敏感，当实例点是噪声时，预测容易出错，也就是说K值过小，整体模型容易产生过拟合。K值过大，则恰恰相反。所以一般用交叉验证法来选取最优的K值</p><p>max_iter： 最大迭代次数，如果聚类很难收敛的话，设置最大迭代次数可以让我们及时得到反馈结果，否则程序运行时间会非常长。</p><p>n_init：初始化中心点的运算次数，默认是 10。程序是否能快速收敛和中心点的选择关系非常大，所以在中心点选择上多花一些时间，来争取整体时间上的快速收敛还是非常值得的。由于每一次中心点都是随机生成的，这样得到的结果就有好有坏，非常不确定，所以要运行 n_init 次, 取其中最好的作为初始的中心点。如果 K 值比较大的时候，你可以适当增大 n_init 这个值。</p><p>init {‘k-means++’, ‘random’, ndarray, callable}, default=’k-means++’： 即初始值选择的方式，默认是采用优化过的 k-means++ 方式，你也可以自己指定中心点，或者采用 random 完全随机的方式。自己设置中心点一般是对于个性化的数据进行设置，很少采用。random 的方式则是完全随机的方式，一般推荐采用优化过的 k-means++ 方式。</p><p>algorithm：k-means 的实现算法，有“auto” “full”“elkan”三种。一般来说建议直接用默认的”auto”。简单说下这三个取值的区别，如果你选择”full”采用的是传统的 K-Means 算法，“auto”会根据数据的特点自动选择是选择“full”还是“elkan”。我们一般选择默认的取值，即“auto” 。</p><h2 id="K-means的小运用"><a href="#K-means的小运用" class="headerlink" title="K-means的小运用"></a>K-means的小运用</h2><p>图像分割：图像分割就是利用图像自身的信息，比如颜色、纹理、形状等特征进行划分，将图像分割成不同的区域，划分出来的每个区域就相当于是对图像中的像素进行了聚类。单个区域内的像素之间的相似度大，不同区域间的像素差异性大。这个特性正好符合聚类的特性，所以你可以把图像分割看成是将图像中的信息进行聚类。当然聚类只是分割图像的一种方式，除了聚类，我们还可以基于图像颜色的阈值进行分割，或者基于图像边缘的信息进行分割等</p><p>例如此图：<br><img src="/2020/07/12/K-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/1.jpg" alt="1"><br>为了处理图像信息，需要获取图像数据，尺寸和通道数，然后基于图像中每个通道的数值进行数据规范化。具体而言代码过程如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> image</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"><span class="comment"># 加载图像，并对数据进行规范化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dealdata</span><span class="params">(filePath)</span>:</span></span><br><span class="line">    f = open(filePath,<span class="string">'rb'</span>)</span><br><span class="line">    data=[]</span><br><span class="line">    <span class="comment"># 得到图像的像素值</span></span><br><span class="line">    img = image.open(f)</span><br><span class="line">    <span class="comment"># 得到图像尺寸</span></span><br><span class="line">    width, height = img.size</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(width):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(height):</span><br><span class="line">            <span class="comment"># 得到点(x,y)的三个通道值</span></span><br><span class="line">            z1, z2, z3 = img.getpixel((x, y))</span><br><span class="line">            data.append([(z1+<span class="number">1</span>)/<span class="number">256.0</span>, (z2+<span class="number">1</span>)/<span class="number">256.0</span>, (z3+<span class="number">1</span>)/<span class="number">256.0</span>])</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> width, height, np.mat(data)</span><br><span class="line"><span class="comment"># 加载图像，得到规范化的结果imgData，以及图像尺寸</span></span><br><span class="line"> width, height ,img= dealdata(<span class="string">'./1.jpg'</span>)</span><br><span class="line"><span class="comment"># 用K-Means对图像进行16聚类</span></span><br><span class="line">kmeans =KMeans(n_clusters=<span class="number">16</span>)</span><br><span class="line">label = kmeans.fit_predict(img)</span><br><span class="line"><span class="comment"># 将图像聚类结果，转化成图像尺寸的矩阵</span></span><br><span class="line">label = label.reshape([width, height])</span><br><span class="line"><span class="comment"># 创建个新图像img，用来保存图像聚类压缩后的结果</span></span><br><span class="line">img=image.new(<span class="string">'L'</span>, (width, height))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(width):</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(height):</span><br><span class="line"><span class="comment"># 根据类别设置图像灰度, 类别0 灰度值为255， 类别1 灰度值为127</span></span><br><span class="line">img.putpixel((x, y), int(<span class="number">256</span> / (label[x][y] + <span class="number">1</span>)) - <span class="number">1</span>)</span><br><span class="line">img.save(<span class="string">'new.jpg'</span>)</span><br></pre></td></tr></table></figure><p>对代码解释如下：</p><p> jpg 格式的图像是三个通道 (R,G,B)，也就是一个像素点具有 3 个特征值。这里用 z1、z2、z3 来获取平面坐标点 (x,y) 的三个特征值，特征值是在 0-255 之间。为了加快聚类的收敛，我采用 Min-Max 规范化对数据进行规范化。我定义的 dealdata 函数返回的结果包括了针对 (R,G,B) 三个通道规范化的数据，以及图像的尺寸信息。我使用了 fit 和 predict 这两个函数来做数据的训练拟合和预测，因为传入的参数是一样的，可以同时进行 fit 和 predict 操作，这样可以直接使用 fit_predict(data) 得到聚类的结果。得到聚类的结果 label 后，实际上是一个一维的向量，需要把它转化成图像尺寸的矩阵。label 的聚类结果是从 0 开始统计的，当聚类数为 2 的时候，聚类的标识 label=0 或者 1。图像聚类的结果进行可视化，直接看 0 和 1 是看不出来的，还需要将 0 和 1 转化为灰度值。灰度值一般是在 0-255 的范围内，我们可以将 label=0 设定为灰度值 255，label=1 设定为灰度值 127。具体方法是用 int(256/(label[x][y]+1))-1。可视化的时候，主要是通过设置图像的灰度值进行显示。所以我把聚类 label=0 的像素点都统一设置灰度值为 255，把聚类 label=1 的像素点都统一设置灰度值为 127。原来图像的灰度值是在 0-255 之间，现在就只有 2 种颜色（也就是灰度为 255，和灰度 127）</p><p>算法执行完效果如下：</p><p><img src="/2020/07/12/K-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/new.jpg" alt="new"></p><p>在实际操作中，我发现不同尺寸的图像，K-Means 运行的时间也是不同的。像素低，算法运行就快，像素高则算法运行就慢。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;K-means的基本原理&quot;&gt;&lt;a href=&quot;#K-means的基本原理&quot; class=&quot;headerlink&quot; title=&quot;K-means的基本原理&quot;&gt;&lt;/a&gt;K-means的基本原理&lt;/h1&gt;&lt;p&gt;K-Means 是一种非监督学习，解决的是聚类问题。K 代表</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>决策树原理学习笔记</title>
    <link href="http://xinzhuo777.github.io/2020/07/11/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://xinzhuo777.github.io/2020/07/11/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8E%9F%E7%90%86%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2020-07-11T13:52:46.000Z</published>
    <updated>2020-07-11T14:08:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树原理学习笔记"><a href="#决策树原理学习笔记" class="headerlink" title="决策树原理学习笔记"></a>决策树原理学习笔记</h1><h2 id="一：构造"><a href="#一：构造" class="headerlink" title="一：构造"></a>一：构造</h2><p>构造的过程就是选择什么属性作为节点的过程，节点分成三种：1.根节点（树的最顶端，最开始的节点）2.内部节点（树的中间节点） 3.叶节点（树的最底部节点，决策的结果）</p><h2 id="二：剪枝"><a href="#二：剪枝" class="headerlink" title="二：剪枝"></a>二：剪枝</h2><p>剪枝可以分为“预剪枝“（Pre-Pruning）和“后剪枝”（Post-Pruning）</p><p>预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。</p><p>后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。</p><p>剪枝是为了防止模型的“过拟合”</p><p>决策树构建中比较重要的三个问题：将哪个属性作为根节点？选择哪些属性作为后继节点？什么时候停止并得到目标值？</p><p>由此引入信息熵这个概念。</p><h2 id="三：信息熵"><a href="#三：信息熵" class="headerlink" title="三：信息熵"></a>三：信息熵</h2><p>信息熵（entropy），它表示了信息的不确定度。在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：</p><script type="math/tex; mode=display">Entropy(t)=-\sum_{i=0}^{c-1}{p(i|t)log_2p(i|t)}</script><p>p(i|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2 为底的对数。这里我们不是来介绍公式的，而是说存在一种度量，它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。</p><p><strong>纯度</strong>把决策树的构造过程理解成为寻找纯净划分的过程。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。</p><p>构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。</p><h2 id="ID3-算法"><a href="#ID3-算法" class="headerlink" title="ID3 算法"></a>ID3 算法</h2><p>ID3 算法计算的是信息增益，信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：</p><script type="math/tex; mode=display">Gain(D,a)=Entropy(D)-\sum_{i=1}^{k}{\frac{|D_i|}{|D|}Entropy(D_i)}</script><p>公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。 ID3 就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树。</p><p>ID3 的算法规则相对简单，可解释性强。同样也存在缺陷，比如我们会发现 ID3 算法倾向于选择取值比较多的属性。有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3 都能生成不错的决策树分类。针对可能发生的缺陷，后人提出了新的算法进行改进</p><h2 id="C4-5-算法"><a href="#C4-5-算法" class="headerlink" title="C4.5 算法"></a>C4.5 算法</h2><ol><li><p>采用信息增益率因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。信息增益率 = 信息增益 / 属性熵，具体的计算公式这里省略。当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。</p></li><li><p>采用悲观剪枝ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。</p></li><li><p>离散化处理连续属性。C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。C4.5 选择具有最高信息增益的划分所对应的阈值。从而达到离散化的效果。</p></li><li><p>处理缺失值。针对数据集不完整的情况，C4.5 也可以进行处理。（采用信息增益率的方式）</p><h4 id="二者对比"><a href="#二者对比" class="headerlink" title="二者对比"></a>二者对比</h4><p>首先 ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。C4.5 在 ID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;决策树原理学习笔记&quot;&gt;&lt;a href=&quot;#决策树原理学习笔记&quot; class=&quot;headerlink&quot; title=&quot;决策树原理学习笔记&quot;&gt;&lt;/a&gt;决策树原理学习笔记&lt;/h1&gt;&lt;h2 id=&quot;一：构造&quot;&gt;&lt;a href=&quot;#一：构造&quot; class=&quot;headerli</summary>
      
    
    
    
    
  </entry>
  
</feed>
